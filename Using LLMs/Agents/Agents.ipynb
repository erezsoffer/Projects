{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UENYwtWFGvJ9",
        "outputId": "14fb44c8-f1e1-4732-f13e-7bced2adefa5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_core\n",
            "  Downloading langchain_core-0.2.7-py3-none-any.whl (315 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/315.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.4/315.6 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m315.6/315.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (6.0.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain_core)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.75 (from langchain_core)\n",
            "  Downloading langsmith-0.1.77-py3-none-any.whl (125 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (2.7.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain_core) (8.3.0)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain_core)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.75->langchain_core)\n",
            "  Downloading orjson-3.10.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m145.0/145.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain_core) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain_core) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain_core) (2024.6.2)\n",
            "Installing collected packages: orjson, jsonpointer, jsonpatch, langsmith, langchain_core\n",
            "Successfully installed jsonpatch-1.33 jsonpointer-3.0.0 langchain_core-0.2.7 langsmith-0.1.77 orjson-3.10.5\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_core"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U42s7zX3G7B0",
        "outputId": "57a233dc-7bec-48d7-a40b-160f33a08211"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langchain_openai\n",
            "  Downloading langchain_openai-0.1.8-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: langchain-core<0.3,>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from langchain_openai) (0.2.7)\n",
            "Collecting openai<2.0.0,>=1.26.0 (from langchain_openai)\n",
            "  Downloading openai-1.34.0-py3-none-any.whl (325 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.5/325.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tiktoken<1,>=0.7 (from langchain_openai)\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (0.1.77)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (2.7.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2.2->langchain_openai) (8.3.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.26.0->langchain_openai) (4.12.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain_openai) (2.31.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.26.0->langchain_openai) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.26.0->langchain_openai)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2.2->langchain_openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2.2->langchain_openai) (3.10.5)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2.2->langchain_openai) (2.18.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain_openai) (2.0.7)\n",
            "Installing collected packages: h11, tiktoken, httpcore, httpx, openai, langchain_openai\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 langchain_openai-0.1.8 openai-1.34.0 tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain_openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCsRRMteHB5Y",
        "outputId": "29917967-ea6e-4992-90f3-b4cd811dcbf9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting langgraph\n",
            "  Downloading langgraph-0.0.69-py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: langchain-core<0.3,>=0.2 in /usr/local/lib/python3.10/dist-packages (from langgraph) (0.2.7)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (6.0.1)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (1.33)\n",
            "Requirement already satisfied: langsmith<0.2.0,>=0.1.75 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (0.1.77)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (24.1)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (2.7.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.3,>=0.2->langgraph) (8.3.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.3,>=0.2->langgraph) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.10.5)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.31.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (2.18.4)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain-core<0.3,>=0.2->langgraph) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langsmith<0.2.0,>=0.1.75->langchain-core<0.3,>=0.2->langgraph) (2024.6.2)\n",
            "Installing collected packages: langgraph\n",
            "Successfully installed langgraph-0.0.69\n"
          ]
        }
      ],
      "source": [
        "!pip install langgraph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hrx-RrDHSVK",
        "outputId": "211cde6f-0cef-4ddd-f650-dbb689d85aab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Executing Agent Initialize\n",
            "Executing Agent GenQueryProgram\n",
            "Executing Agent ExecuteProgram, count: 1\n",
            "Executing Agent Chk4rErr\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import json\n",
        "import io\n",
        "import sys\n",
        "from contextlib import redirect_stdout\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "from langgraph.graph import StateGraph, END\n",
        "from typing import TypedDict\n",
        "\n",
        "# Set up Azure OpenAI API key and endpoint\n",
        "AZURE_OPENAI_API_KEY = 'AZURE_OPENAI_API_KEY'  # Replace with your AZURE_OPENAI_API_KEY\n",
        "AZURE_OPENAI_ENDPOINT = 'AZURE_OPENAI_ENDPOINT'  # Replace with your AZURE_OPENAI_ENDPOINT\n",
        "OPENAI_API_VERSION = '2024-02-01'\n",
        "\n",
        "model = AzureChatOpenAI(\n",
        "    openai_api_version=OPENAI_API_VERSION,\n",
        "    azure_deployment='gpt4',\n",
        "    azure_endpoint=AZURE_OPENAI_ENDPOINT,\n",
        "    openai_api_key=AZURE_OPENAI_API_KEY,\n",
        ")\n",
        "\n",
        "class GraphState(TypedDict):\n",
        "    query_name: str\n",
        "    data_file: str\n",
        "    columns: str\n",
        "    query: str\n",
        "    program_code: str\n",
        "    program_output: str\n",
        "    error_message: str\n",
        "    reflection: str\n",
        "    execution_count: int\n",
        "    terminate: bool\n",
        "\n",
        "# Initialize the state with input data from files\n",
        "def initialize(state):\n",
        "    print(\"Executing Agent Initialize\")\n",
        "    # Read the initial input file\n",
        "    with open('query_input.txt', 'r') as f:\n",
        "        lines = f.readlines()\n",
        "    state['query_name'] = lines[0].split(':')[1].strip()\n",
        "    state['data_file'] = lines[1].split(':')[1].strip()\n",
        "    state['columns'] = lines[2].split(':')[1].strip()\n",
        "\n",
        "    # Read the query from the corresponding query file\n",
        "    query_file = f\"{state['query_name']}_query.txt\"\n",
        "    with open(query_file, 'r') as f:\n",
        "        state['query'] = f.read().strip()\n",
        "\n",
        "    state['execution_count'] = 0\n",
        "    state['terminate'] = False\n",
        "    return state\n",
        "\n",
        "# Extract Python code block from the response content\n",
        "def extract_code_from_response(response_content):\n",
        "    code_block = re.search(r'```python(.*?)```', response_content, re.DOTALL)\n",
        "    if code_block:\n",
        "        return code_block.group(1).strip()\n",
        "    return response_content.strip()\n",
        "\n",
        "# Generate the initial Python program to query the CSV file\n",
        "def gen_query_program(state):\n",
        "    print(\"Executing Agent GenQueryProgram\")\n",
        "    prompt = f\"\"\"\n",
        "    Write a Python program to perform the following query on a CSV file:\n",
        "    {state['query']}\n",
        "    The CSV file is named {state['data_file']} and contains the following columns: {state['columns']}.\n",
        "    The program should output a JSON structure as specified in the query.\n",
        "    \"\"\"\n",
        "    response = model.invoke([HumanMessage(content=prompt)])\n",
        "    state['program_code'] = extract_code_from_response(response.content)\n",
        "    return state\n",
        "\n",
        "# Execute the generated Python program and capture its output\n",
        "def execute_program(state):\n",
        "    print(f\"Executing Agent ExecuteProgram, count: {state['execution_count'] + 1}\")\n",
        "    state['execution_count'] += 1\n",
        "    program_code = state['program_code']\n",
        "    try:\n",
        "        exec_globals = {}\n",
        "        with io.StringIO() as buf, redirect_stdout(buf):\n",
        "            exec(program_code, exec_globals)\n",
        "            output = buf.getvalue()\n",
        "        state['program_output'] = output.strip()\n",
        "    except Exception as e:\n",
        "        state['program_output'] = \"\"\n",
        "        state['error_message'] = str(e)\n",
        "    return state\n",
        "\n",
        "# Check for errors in the program's output\n",
        "def chk4r_err(state):\n",
        "    print(\"Executing Agent Chk4rErr\")\n",
        "    if not state['program_output']:\n",
        "        state['terminate'] = True\n",
        "    else:\n",
        "        try:\n",
        "            json.loads(state['program_output'])\n",
        "            state['terminate'] = False\n",
        "        except json.JSONDecodeError:\n",
        "            state['terminate'] = True\n",
        "    return state\n",
        "\n",
        "# Reflect on the errors and suggest a fix\n",
        "def reflect_on_err(state):\n",
        "    print(\"Executing Agent ReflectOnErr\")\n",
        "    reflection_prompt = f\"\"\"\n",
        "    The following Python code failed to execute correctly:\n",
        "    {state['program_code']}\n",
        "    It produced the following error message: {state['error_message']}\n",
        "    Reflect on why this error occurred and suggest a way to fix it.\n",
        "    \"\"\"\n",
        "    reflection_response = model.invoke([HumanMessage(content=reflection_prompt)])\n",
        "    state['reflection'] = reflection_response.content\n",
        "    return state\n",
        "\n",
        "# Regenerate the Python program based on the reflection\n",
        "def regen_query_pgm(state):\n",
        "    print(\"Executing Agent ReGenQueryPgm\")\n",
        "    regen_prompt = f\"\"\"\n",
        "    Based on the following reflection, regenerate the Python program to fix the errors:\n",
        "    {state['reflection']}\n",
        "    Original prompt: Write a Python program to perform the following query on a CSV file:\n",
        "    {state['query']}\n",
        "    The CSV file is named {state['data_file']} and contains the following columns: {state['columns']}.\n",
        "    The program should output a JSON structure as specified in the query.\n",
        "    \"\"\"\n",
        "    regen_response = model.invoke([HumanMessage(content=regen_prompt)])\n",
        "    state['program_code'] = extract_code_from_response(regen_response.content)\n",
        "    return state\n",
        "\n",
        "# Decide the next state based on the current execution state\n",
        "def decide_next_state(state):\n",
        "    if state['execution_count'] >= 3:\n",
        "        state['terminate'] = True\n",
        "        return END\n",
        "    if state['terminate']:\n",
        "        return \"ReflectOnErr\"\n",
        "    return END\n",
        "\n",
        "# Build the LangGraph workflow\n",
        "workflow = StateGraph(GraphState)\n",
        "workflow.add_node(\"Initialize\", initialize)\n",
        "workflow.add_node(\"GenQueryProgram\", gen_query_program)\n",
        "workflow.add_node(\"ExecuteProgram\", execute_program)\n",
        "workflow.add_node(\"Chk4rErr\", chk4r_err)\n",
        "workflow.add_node(\"ReflectOnErr\", reflect_on_err)\n",
        "workflow.add_node(\"ReGenQueryPgm\", regen_query_pgm)\n",
        "\n",
        "# Add edges between nodes\n",
        "workflow.add_edge(\"Initialize\", \"GenQueryProgram\")\n",
        "workflow.add_edge(\"GenQueryProgram\", \"ExecuteProgram\")\n",
        "workflow.add_edge(\"ExecuteProgram\", \"Chk4rErr\")\n",
        "workflow.add_conditional_edges(\"Chk4rErr\", decide_next_state, {\"ReflectOnErr\": \"ReflectOnErr\", END: END})\n",
        "\n",
        "# Reflect and regenerate\n",
        "workflow.add_edge(\"ReflectOnErr\", \"ReGenQueryPgm\")\n",
        "workflow.add_edge(\"ReGenQueryPgm\", \"ExecuteProgram\")\n",
        "\n",
        "# Set entry point\n",
        "workflow.set_entry_point(\"Initialize\")\n",
        "\n",
        "# Compile and run the workflow\n",
        "runnable = workflow.compile()\n",
        "initial_state = GraphState(\n",
        "    query_name=\"\", data_file=\"\", columns=\"\", query=\"\", program_code=\"\",\n",
        "    program_output=\"\", error_message=\"\", reflection=\"\", execution_count=0, terminate=False\n",
        ")\n",
        "\n",
        "# Execute the workflow\n",
        "result = runnable.invoke(initial_state)\n",
        "\n",
        "# Handle final output\n",
        "with open(f\"{result['query_name']}.py\", 'w') as f:\n",
        "    f.write(result['program_code'])\n",
        "with open(f\"{result['query_name']}.txt\", 'w') as f:\n",
        "    f.write(result['program_output'])\n",
        "with open(f\"{result['query_name']}_errors.txt\", 'w') as f:\n",
        "    f.write(result['error_message'])\n",
        "with open(f\"{result['query_name']}_reflect.txt\", 'w') as f:\n",
        "    f.write(result['reflection'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJQGfPR4JlIL"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
